<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Text segmentation using DETR | Mattia  Bertè</title>
    <meta name="author" content="Mattia  Bertè">
    <meta name="description" content="Text segmentation problem solved using DETR architecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tiaberte.github.io/projects/text-segmentation/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mattia </span>Bertè</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Text segmentation using DETR</h1>
            <p class="post-description">Text segmentation problem solved using DETR architecture</p>
          </header>

          <article>
            <h1 id="project-description">Project description</h1>
<p>Less than a third of high school seniors in the U.S. are proficient writers, according to the National Assessment of Educational Progress1. Low-income, Black, and Hispanic students fare even worse, with less than 15 percent demonstrating writing proficiency. Since good writing skills are important for success, unbiased support for writing could have a positive impact on society. One way to achieve this is via automated feedback tools, which often require a pipeline of tools to be deployed. We decided to contribute to the cause, building and releasing a model for text segmentation and argumentative element classification. Our model is an end-to-end neural network based on Transformers, capable of identifying the argumentative elements of a text and classifying them. We address the problem, proposing a new architecture inspired by successful ones for object detection in Computer Vision. Regarding the dataset, we used the one provided by the Feedback Prize competition on Kaggle.
The code can be found on <a href="https://github.com/TiaBerte/text-segmentation" rel="external nofollow noopener" target="_blank">GitHub</a>.</p>

<h1 id="table-of-contents">Table of Contents</h1>
<ol>
  <li><a href="#section1">Background</a></li>
  <li><a href="#section2">System description</a></li>
  <li><a href="#section3">Architecture</a></li>
  <li><a href="#section4">From Object Detection to Text Segmentation</a></li>
  <li><a href="#section5">Evaluation metrics</a></li>
  <li><a href="#section6">Experimental setup</a></li>
  <li><a href="#section7">Results</a></li>
  <li><a href="#section8">Error analysis</a></li>
  <li><a href="#section9">Discussion</a></li>
</ol>

<h2 id="background--">Background  <a name="section1"></a>
</h2>
<h3 id="text-segmentation">Text Segmentation</h3>
<p>Text Segmentation is an underexplored area of research. The goal of this task is to identify argumentative elements from a document. Closely related is the <em>_Argument Mining</em> task, from which we take inspiration. In the latter, the aim is also to extract a structure of argumentative elements, while in our task this is not necessary (even if an understanding of the structure of the text can help achieve better performances). One notable reference is <a href="https://arxiv.org/pdf/2011.13187.pdf" rel="external nofollow noopener" target="_blank">(Ruiz et al., 2021)</a> which shows the effectiveness of transformer-based architectures for the Argument Mining tasks. <a href="https://arxiv.org/ftp/arxiv/papers/2102/2102.12227.pdf" rel="external nofollow noopener" target="_blank">(Galassi et al., 2021)</a> proposed <em>ResAttArg</em>, a state-of-the-art architecture for Augmentative Mining. The main limitation is that it doesn’t scale up well with large documents. We aim to overcome this issue using <a href="https://arxiv.org/pdf/2004.05150.pdf" rel="external nofollow noopener" target="_blank">Longformer</a>.</p>

<h3 id="longformer">Longformer</h3>
<p>Longformers are a particular type of transformer proposed to scale up for long documents. To do so, they use two types of attention, a local and a global one. The local attention is standard sliding-window attention, while the global one is a sparse pattern to attend to all the documents. With this trick, the scales linearly with the size of the sequence to process, making it suitable for our task. However, choosing the global attention pattern is a new implementation choice that must be tuned.</p>

<h3 id="detr">DETR</h3>
<p><a href="https://arxiv.org/pdf/2005.12872.pdf" rel="external nofollow noopener" target="_blank">DETR</a> is a state-of-the-art architecture for object detection proposed by FAIR. In a nutshell, it involves a backbone, typically a ResNet50 or a ResNet101 , that extracts the important feature of the image. These features are passed to an encoder-decoder transformer that extracts the box predictions. In its simplicity, DETR doesn’t require a complex and hand-crafted pipeline for box extraction and it is highly parallelizable. However, it requires setting a maximum number of box predictions. For training, DETR uses two different losses. The first one is the cross-entropy loss for the classification of the boxes. The second one is a linear combination of the \(L_1\) norm between the predicted center and length of the bounding boxes and the ground truth plus the Generalized Intersection-Over-Union between the true and the predicted boxes. <br>
To match the boxes with the ground truth, DETR uses the Hungarian algorithm. So the overall loss is:</p>

\[\mathcal{L}_{Hungarian}(y, \hat{y}) = \sum^N_{i=0}\big[ -\lambda_{CE} \log p_{\hat{\sigma}(i)(c_i)} + 1_{\{c_i \neq \emptyset\}} \mathcal{L}_{box}(b_i, \hat{b}_{\hat{\sigma}(i)}) \big]\]

<p>where \(\lambda_{CE}\) is a hyperparameter and \(\hat{\sigma}\) is the optimal assignment computed as:</p>

\[\hat{\sigma} = argmin_{\sigma \in \Theta} \sum_i^N \mathcal{L}_{match}(y_i, \hat{y}_{\sigma_i})\]

<p>and \(\mathcal{L}_{box}\) is:</p>

\[\mathcal{L}_{box}(b_i, \hat{b}_{\sigma_i}) = \lambda_{iou}\mathcal{L}_{iou}(b_i, \hat{b}_{\sigma_i}) + \lambda_{L1}||(b_i, \hat{b}_{\sigma_i})||_1\]

<p>with \(\lambda_{iou}\) and \(\lambda_{L1}\) hyperparameters.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/detr2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/detr2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/detr2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/detr2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
     DETR architecture representation. The model predicts a fixed number of boxes and then classifies them with a special class to discard boxes.
</div>

<h2 id="system-description-">System description <a name="section2"></a>
</h2>

<h3 id="dataset">Dataset</h3>
<p>The dataset contains more than 15k argumentative essays written by U.S students in grades 6-12. The essays were annotated by expert raters for elements commonly found in argumentative writing.<br>
As always in kaggle competition, the test set is private, you have access to a certain amount of evaluations of your model, however we decided to create also the test set from the whole dataset. Doing so we could analyze the error of our model on the test set, which wouldn’t be possible otherwise.<br>
As shown in figure, the documents have a quite spread distribution of lengths, with the majority having less than 1000 words. Instead, the number of argumentative units in each document follows a Gaussian distribution.</p>

<div class="row">
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/documents_length_his.pdf-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/documents_length_his.pdf-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/documents_length_his.pdf-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/documents_length_his.pdf" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/arg_units_per_doc_hist.pdf-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/arg_units_per_doc_hist.pdf-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/arg_units_per_doc_hist.pdf-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/arg_units_per_doc_hist.pdf" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Number of words in each document and number of argumentative units for each document.
</div>

<h3 id="annotations">Annotations</h3>
<p>The annotations report the following information:</p>
<ul>
  <li>
<em>discourse_start</em> - character position where discourse element begins in the essay response;</li>
  <li>
<em>discourse_end</em> - character position where discourse element ends in the essay response;</li>
  <li>
<em>discourse_type</em> - classification of discourse element;</li>
</ul>

<p>Those kinds of annotation are similar to the ones commonly found in object detection tasks. In fact, we can make a parallelism between pixel position in the images of bounding boxes, and character position in the document of the discourse units for details. The major difference is that here the segmentation boxes are 1-dimensional, instead of the 2-D bounding boxes in image object detection. So we augment the annotations by including two new features:</p>
<ul>
  <li>
<em>box_center</em> - character position of the center of the discourse;</li>
  <li>
<em>box_length</em> - number of characters in the discourse;</li>
</ul>

<p>Those are the features that our model will train on and predict.</p>

<h3 id="classes">Classes</h3>
<p>The discourses are classified using the following categorization:</p>

<ul>
  <li>
<strong>Lead</strong> - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader’s attention and point toward the thesis;</li>
  <li>
<strong>Position</strong> - an opinion or conclusion on the main question;</li>
  <li>
<strong>Claim</strong> - a claim that supports the position;</li>
  <li>
<strong>Counterclaim</strong> - a claim that refutes another claim or gives an opposing reason to the position;</li>
  <li>
<strong>Rebuttal</strong> - a claim that refutes a counterclaim;</li>
  <li>
<strong>Evidence</strong> - ideas or examples that support claims, counterclaims, or rebuttals;</li>
  <li>
<strong>Concluding Statement</strong> - a concluding statement that restates the claims;</li>
</ul>

<p>Some parts of the essays will be unannotated (i.e., they do not fit into one of the classifications above).</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/class_freqs-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/class_freqs-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/class_freqs-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/class_freqs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Distribution of the classes.
</div>

<p>The classes <em>Claim</em> and  <em>Evidence</em> are by far the most common in the dataset.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/discourse_rel_pos-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/discourse_rel_pos-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/discourse_rel_pos-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/discourse_rel_pos.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Average start position, end position and length (in characters) normalized with respect to the document length (with standard deviations)
</div>

<p>In figure  we can see that on average the documents respect the structure of the argumentative essay. They start with the lead section, to introduce the thesis, followed by positions, claims and evidence to support the thesis. Then comes the counterclaims and the rebuttals, that are needed to give more credibility to the thesis, and then they end with the concluding statements. <br>
The central sections have a large standard deviation, as they can be mixed and even repeated multiple times in an essay. As we can expect, the larger sections are the Lead, the Concluding statements and the Evidence because they are the main parts of an argumentative essay, instead, the other sections are just needed to introduce the discussion.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_class_correlation-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_class_correlation-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_class_correlation-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/word_class_correlation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Correlation between words and classes.
</div>

<p>We also evaluated how the words correlate with the classes. As correlation metrics, we take inspiration from the Tf-idf statistic, which is intended to reflect how important a word is to a document in a collection or corpus. We evaluated the term frequency in the class as.</p>

\[\mathrm {tf_{c, t}} = \frac{f_{c, t}}{\sum_t' f_{c, t'}}\]

<p>with \(f_{c, t}\) as the counts of the occurrence of the term \(t\) in the class \(c\). Then instead of the inverse-document-frequency term, we weighted the frequencies with an inverse-discourse-frequency in as</p>

\[\mathrm {idf_t} = \log\frac{|D|}{|\{d \in D : t \in d\}|}\]

<p>with \(D\) the set of all discourses in the dataset.<br>
Some interesting correlations emerge. For example, in the <em>Counterclaim</em> class, the most correlated are <em>argue, say, although, may, might</em>, which are all words that introduce a hypothetical opposite opinion. In <em>Position</em> we can find words like <em>believe, idea, agree</em>, instead the top correlation for <em>Rebuttal</em> and <em>Concluding Statement</em> are respectively <em>however</em> and <em>conclusion</em>, as one might expect. Instead <em>Claim</em>, <em>Evidence</em> and <em>Lead</em> correlate with more generic words, maybe because in those classes there is more freedom in the content.</p>

<h2 id="architecture-">Architecture <a name="section3"></a>
</h2>
<p>Starting from the DETR proposal, we tried to adapt this architecture to our task. Even if CNNs still represent the SoTA in some computer vision tasks, they don’t work so well in NLP tasks, where transformers became the dominant paradigm in the last years. Following these premises, we decide to replace the ResNet feature extractor in favour of an advanced version of transformers called Longformer. Actually, the Longformer architecture is nothing different from the BERT model, the innovative idea is related to the attention mechanism. The original Transformer model has a self-attention component with \(O(n^2)\) time and memory complexity where $n$ is the input sequence length, the Longformer model instead tries to combine local attention with a sliding windows approach, whose complexity is \(O(n)\), plus global attention related to preselected input locations in order to provide inductive bias to the model, since the number of these inputs is small related to the length of the text, the model complexity is still \(O(n)\). Since our task requires dealing with long texts, in which each sentence has to be analyzed with respect to the whole text, Longformer is a good trade-off.
Following the same intuition, since the Longformer is present in the encoder-decoder variant, we decided to use it also for the second part of the architecture.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/attention-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/attention-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/attention-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/attention.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Comparison between Longformer attention mechanism and previous proposal.
</div>

<h2 id="from-object-detection-to-text-segmentation-">From Object Detection to Text Segmentation <a name="section4"></a>
</h2>
<p>As said before we have made parallelism between object detection and text segmentation. The main difference is that in object detection the boxes are 2-D, instead in text segmentation they are 1-D. The regression head of DETR predicts 4 parameters that are the center and the dimensions of the bounding boxes, normalized in the range \((0, 1)\) with respect to the dimensions of the image. We changed it to predict only 2 parameters, the center and the length of the “segmentation boxes”. We choose to consider those values normalized with respect to the position of the words in the document tokenized by the LongormerTokenizer that we used during training.<br>
With this in mind, we adapted all the operations that should have involved the 2-D bounding boxes, in order to make them work with our 1-dimensional “boxes”. The main function to modify is the \(\mathcal{L}_{iou}\) which evaluates the Intersection Over Union between bounding boxes, and it’s used both in the Hungarian Matcher and in the Loss function. In particular, we used the <a href="https://arxiv.org/pdf/1902.09630.pdf" rel="external nofollow noopener" target="_blank">Generalized Intersection Over Union</a>, that overcomes the issues of the plateau that IoU has in the case of nonoverlapping bounding boxes. GIoU in the general case is defined as follows: for two arbitrary convex shapes (volumes) \(A, B \subseteq \mathbb{S} \in \mathbb{R}^n\), find the smallest convex shapes \(C \subseteq \mathbb{S} \in \mathbb{R}^n\) enclosing both \(A\) and \(B\). Then calculate a ratio between the volume (area) occupied by \(C\) excluding \(A\) and \(B\) and divide by the total volume (area) occupied by \(C\). This represents a normalized measure that focuses on the empty volume (area) between \(A\) and \(B\). Finally, GIoU is attained by subtracting this ratio from the IoU value. In summary:</p>

\[GIoU = \frac{|A \cap B|}{|A \cup B|} - \frac{|C \setminus A \cup B|}{|C|}\]

<p>\(GIoU\) is both a metric and a loss, with \(\mathcal{L}_{GIoU}(A, B) = 1 - GIoU(A, B)\). It shows a consistent improvement in performance measures on popular object detection benchmarks by incorporating this generalized IoU as a loss into the state-of-the-art object detection frameworks. <br>
To adapt this loss to our task, we considered to have boxes in the start-end format \(b_i = (s_i, e_i)\) obtained from the predictions in format center-length with the equations \(s_i = c_i - \frac{l_i}{2}\) and \(e_i = c_i + \frac{l_i}{2}\) so it’s guaranteed that \(s_i \leq e_i\). We evaluate the \(GIoU\) as follows:</p>

<ul>
  <li>\(\| b_i \| = l_i = s_i - e_i\) - The area of the boxes is their length;</li>
  <li>\(\| b_i \cap b_j \| = max(0, max(s_i, s_j) - min(e_i, e_j))\) - The intersection of boxes is the length of the segment they have in common;</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$</td>
          <td>b_i \cup b_j</td>
          <td>=</td>
          <td>b_i</td>
          <td>+</td>
          <td>b_j</td>
          <td>-</td>
          <td>b_i \cap b_j</td>
          <td>$$;</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$</td>
          <td>C</td>
          <td>= min(s_i, s_j) - max(e_i, e_j)$$;</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$</td>
          <td>C \setminus b_i \cup b_j</td>
          <td>=</td>
          <td>C</td>
          <td>-</td>
          <td>b_i \cup b_j</td>
          <td>$$ - The length of the segment that, if any, separates $b_i$ and $b_j$. With perfect overlap, this factor is equal to 0.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>Since every token can belong to at most one argumentative part, we also experiment with a new loss \(\mathcal{L}_{Overlap}\) that penalizes overlapping between the boxes issued by the network. Thus this loss is simply the mean of the \textit{GIoU} between every couple of boxes issued by the network.</p>

<h2 id="evaluation-metrics-">Evaluation metrics <a name="section5"></a>
</h2>

<p>To evaluate our model we used the metric described in the competition web page. These metrics evaluate the overlap between ground truth and predicted word indices.<br>
For each sample, all ground truths and predictions for a given class are compared.<br>
If the overlap between the ground truth and prediction is \(\geq 0.5\), and the overlap between the prediction and the ground truth \(\geq 0.5\), the prediction is a match and is considered a true positive. If multiple matches exist, the match with the highest pair of overlaps is taken.<br>
Any unmatched ground truths are false negatives and any unmatched predictions are false positives.<br>
The final score is arrived at by calculating TP/FP/FN for each class, then taking the macro F1 score across all classes.<br>
Annotations and predictions must be in word indices, that is calculated by using Python’s \verb|.split()| function and taking the indices of the words of the discourse unit in the resulting list. The two overlaps are calculated by taking the  \verb|set()| of each list of indices in a ground truth/prediction pair and calculating the intersection between the two sets divided by the length of each set.<br>
The idea is similar to the Mean Average Precision (mAP) used commonly in object detection tasks, for example to evaluate the accuracy in the COCO dataset, where a threshold on the Intersection Over Union (IOU) is used to evaluate the matches between ground truth and predicted bounding boxes.<br>
For example, if for the class <em>Claim</em> in a document there are ground truth annotations \([(1, 2, 3, 4, 5), (6, 7, 8), (21, 22, 23, 24, 25)]\) and the model predicts \([(1, 2), (6, 7, 8)]\) then:</p>

<ul>
  <li>The first prediction would not have \(\geq 0.5\) overlap with either ground truth and would be a false positive.</li>
  <li>The second prediction would overlap perfectly with the second ground truth and be a true positive.</li>
  <li>The third ground truth would be unmatched, and would be a false negative.</li>
</ul>

<h2 id="experimental-setup-">Experimental setup <a name="section6"></a>
</h2>

<p>We trained our networks on a Tesla V100. The training takes about 30 min per epochs. We trained around 50 networks for tuning purposes, with an average of 2 epochs each, resulting in roughly 50 hours of training.   <br>
We experimented multiple times varying the setup to understand the best way for training this model. For tackling the class imbalance problem, we implemented the focal loss technique. Then we tried also different initialization of weights and bias for the final layers and also for the encoder-decoder model when non-pretrained weights were used. We experimented also with fine-tuning of the features extractor backbone and with it frozen. As optimizer, we used \textit{AdamW} and we dropped the learning rates for the second epochs by a factor of ten.
We report the best hyperparameters that we found:</p>

<ul>
  <li>Hidden dimension = 2048;</li>
  <li>Heads lr = \(10^{-4}\);</li>
  <li>Transf. lr = \(10^{-5}\);</li>
  <li>Number of queries = 40;</li>
  <li>\(\lambda_{CE}\) = 1;</li>
  <li>\(\lambda_{L1}\) = 1;</li>
  <li>\(\lambda_{GIoU}\) = 0.5;</li>
  <li>\(\lambda_{Overlap}\) = 0.5;</li>
  <li>\(\gamma_{FL}\) = 2.</li>
</ul>

<p>where \(\lambda\)s are the coefficient of the diverse components of the loss as indicated by the subscript and \(\gamma_{FL}\) is the modulating term of the cross entropy (focal loss).
For the best model, we also set the depth for the classification head and the bounding box head to 3 and 5 layers respectively. We also initialize the bias of the last classification layer in order to match the prior distribution of classes.<br>
Instead, we don’t apply weight decay nor dropout, since our model doesn’t overfit the training data. We didn’t see any benefit in applying or not gradient clipping.<br>
The model has 235M parameters and was trained for 2 epochs since training for more time doesn’t improve its performance.  <br>
In order to exploit the information obtained from the data set analysis, we decided to define a new strategy for the global attention. For each class we detected the most frequent word and associated with them the token of the global attention, to impose the network a certain focus on particularly relevant words.</p>

<p>\begin{table}[ht]
\centering
\begin{tabular<em>}{.45\linewidth}{@{\extracolsep{\fill}}cc}
\hline
\textbf{Class} &amp; \textbf{Most freq. word}<br>
\hline
Claim &amp; Reason <br>
Concluding statement &amp; Conclusion<br>
Counterclaim &amp; Argue<br>
Evidence &amp; Electors<br>
\hline
\end{tabular</em>}
\quad
\begin{tabular<em>}{.45\linewidth}{@{\extracolsep{\fill}}cc}
\hline
\textbf{Class} &amp; \textbf{Most freq. word}<br>
\hline
Lead &amp; Name<br>
Position &amp; Believe<br>
Rebuttal &amp; However<br>
 &amp; <br>
\hline
\end{tabular</em>}
\caption{Each class with its correspondent most frequent word.}
\label{table:class_words}
\end{table}</p>

<p>As can be seen, in some cases we can notice a relation between the meaning of a word and its classes, while in other cases we have words whose presence is strictly dependent on our data set (Lead/Name, Evidence/Electors). However for sake of fairness, we selected one word for each class.</p>

<h2 id="results-">Results <a name="section7"></a>
</h2>

<p>As baseline we thought to use a simple detector which predict always the same class, ‘Claim’ which is the most frequent and detect an argumentative element every 151 words which is the median length. It can give a qualitative information on the hardness of the task and it is useful to show how we were able to improve this simplest classifier. For a standard classifier defining a simple baseline it’s easy, it’s not the same for an object detection problem, indeed the baseline we proposed isn’t really informative.</p>

<p>\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Class} &amp; \textbf{Precision}  &amp; \textbf{Recall}  &amp; \textbf{F1-score}<br>
\hline
Lead                 &amp;  0.000 &amp; 0.000 &amp; 0.000<br>
Position             &amp;  0.000 &amp; 0.000 &amp; 0.000<br>
Evidence             &amp;  0.000 &amp; 0.000 &amp; 0.000<br>
Claim                &amp;  0.001 &amp; 0.001 &amp; 0.001<br>
Concluding Statement &amp;  0.000 &amp; 0.000 &amp; 0.000<br>
Counterclaim         &amp;  0.000 &amp; 0.000 &amp; 0.000<br>
Rebuttal             &amp;  0.000 &amp; 0.000 &amp; 0.000<br>
\hline
\textbf{Macro Avg}   &amp;  0.0001 &amp; 0.0002 &amp; 0.0002<br>
\hline
\end{tabular}
\caption{Scores of the baseline model.}
\end{table}</p>

<p>\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Class} &amp; \textbf{Precision}  &amp; \textbf{Recall}  &amp; \textbf{F1-score}<br>
\hline
Lead                 &amp;  0.418 &amp; 0.705 &amp; 0.525<br>
Position             &amp;  0.219 &amp; 0.222 &amp; 0.221<br>
Evidence             &amp;  0.559 &amp; 0.567 &amp; 0.563<br>
Claim                &amp;  0.137 &amp; 0.342 &amp; 0.196<br>
Concluding Statement &amp;  0.213 &amp; 0.840 &amp; 0.340<br>
Counterclaim         &amp;  0.019 &amp; 0.165 &amp; 0.035<br>
Rebuttal             &amp;  0.019 &amp; 0.036 &amp; 0.025<br>
\hline
\textbf{Macro Avg}   &amp;  0.226 &amp; 0.411 &amp; 0.272<br>
\hline
\end{tabular}
\caption{Scores of the best model without global attention on most frequent words for each class.}
\end{table}</p>

<p>\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Class} &amp; \textbf{Precision}  &amp; \textbf{Recall}  &amp; \textbf{F1-score}<br>
\hline
Lead                 &amp;  0.438 &amp; 0.715 &amp; 0.543<br>
Position             &amp;  0.179 &amp; 0.389 &amp; 0.246<br>
Evidence             &amp;  0.371 &amp; 0.661 &amp; 0.476<br>
Claim                &amp;  0.119 &amp; 0.256 &amp; 0.163<br>
Concluding Statement &amp;  0.535 &amp; 0.615 &amp; 0.572<br>
Counterclaim         &amp;  0.015 &amp; 0.068 &amp; 0.025<br>
Rebuttal             &amp;  0.010 &amp; 0.066 &amp; 0.017<br>
\hline
\textbf{Macro Avg}   &amp;  0.238 &amp; 0.396 &amp; 0.292<br>
\hline
\end{tabular}
\caption{Scores of the best model with global attention on most frequent words for each class.}
\end{table}</p>

<p>Adding the global attention to particular words present a general improvement \((+6\%\) in f1-score). However we can notice a drop in performances related to the recall while an increasing in the precision. This probably means that the network lose a bit in the capability of classification but improve its performances in the identification of the position of the argumentative units. This technique could be in general a good idea, however some of the words we used don’t present any real correlation with their class. So it is highly probable that in a different scenario using this words could lead to a poor results. The way in which we selected them is too task dependant, another possibility would be to use a list of words provided by a team of experts in order to have a more general capability.</p>

<h2 id="error-analysis-">Error analysis <a name="section8"></a>
</h2>
<p>One of the most common error and easiest to notice is the incapability of the model to understand which are the most common words or symbols that define the beginning and the end of a sentence, such as periods, commas or conjunction which usually are very helpful also for humans for understanding which kind of argumentative unit we can expect. A possible way to influence the model towards this direction, could be to use the global attention on all the conjunction. On one hand, we have that this approach would be very informative for the model since it would mimick also the way in which human annotators establish the groundtruth, on the other hand there would be the risk of increasing too much the computational complexity.
Another common error is the overlapping between different detection, in some cases it is related just to consecutive detection, in other cases instead it is due to multiple detection of the same unit, this problem is pretty common in object detection task. A possible way to overcome this problem, would be to adopt technique such as non-maxima suppression, which consist in discarding all the detection which overlap more than a threshold and keeping just the one with the greatest score. Clearly also this approach doesn’t come for free, it increases the computational complexity and moreover it takes in account only the best score, without any consideration for the rest. Below you can see an example the perfectly summarize the errors described above.<br>
\({\color{magenta}
\text{participate seagoing cowboys once. People might not have many reasons to support there statement about 'Why you [Claim]}
}\)
\({\color{blue}
\text{participate seagoing cowboys once. People might not have many reasons to support there  statement about 'Why you shouldn't be a Seagoing Cowboy', but I have at least some   reasons as why you should join [Evidence]}
}\)</p>

<p>In the end, another big difference between the object detection task and ours, is related to the proportion foreground/background and Argumentative/Non-Argumentative units. In the first case is common to have small number of object with a lot of background while in pour case, working with argumentative essay, more or less all the proposition have to be classified.</p>

<h2 id="discussion-">Discussion <a name="section9"></a>
</h2>

<p>Even before starting, we were conscious that trying to tackle such a complex task as Argumentative Mining using a completely different approach, in particular one derived from object detection, would have been an hard challenge. However we were curious to explore new path and propose a different point of view for this kind of problems. It gave us the opportunity to work and understand not only the way in which transformers work, but also how they can be optimized for handling long texts, which by the way can be very useful for argumentative essays.
This problem was very stimulating and challenging. Even if it’s not new, the fact that doesn’t exist a common solution which is able to perform well in any situation, urge us to analyze in deep the data set to find different way to improve our initial solution. Of course we wouldn’t expect to tie the results of the top teams of the competition, we were more interested in proposing something new, or at least not so spread as more standard approaches. We hope it could offer a new perspective to Argumentative task and maybe to find a direction to merge CV and NLP with model capable of both tasks at the same time.</p>

          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Mattia  Bertè. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
