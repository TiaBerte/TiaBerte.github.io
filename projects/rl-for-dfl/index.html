<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>RL for Decision Focused Learning | Mattia  Bertè</title>
    <meta name="author" content="Mattia  Bertè">
    <meta name="description" content="Analysis of RL algorithm in the context of Decision Making">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tiaberte.github.io/projects/rl-for-dfl/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mattia </span>Bertè</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/"></a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">RL for Decision Focused Learning</h1>
            <p class="post-description">Analysis of RL algorithm in the context of Decision Making</p>
          </header>

          <article>
            <h1 id="project-description">Project description</h1>
<p>This project contains an analysis of the soft actor-critic algorithm applied in the field of decision-focused learning for solving the set multicover problem. We experimented a variation using prioritized experience replay for improving convergence. The algorithm and the environments were mainly derived from the garage library and can be found on <a href="https://github.com/TiaBerte/rl-for-dfl" rel="external nofollow noopener" target="_blank">GitHub</a>.</p>

<h1 id="table-of-contents">Table of Contents</h1>
<ol>
  <li><a href="#section1">Decision Focused Learning</a></li>
  <li><a href="#section2">Set Multicover Problem</a></li>
  <li><a href="#section3">Soft Actor Critic</a></li>
  <li><a href="#section4">Prioritized Experience Replay</a></li>
  <li><a href="#section5">Experiments</a></li>
  <li><a href="#section6">Conclusions</a></li>
</ol>

<h1 id="decision-focused-learning--">Decision Focused Learning  <a name="section1"></a>
</h1>
<p>Decision-making algorithms usually require predictive models and a combinatorial optimization phase, however, these two tasks are often tackled independently following the so-called “Predict and Optimize” paradigm which consists in training a machine learning model to maximize the prediction accuracy, and then using the prediction as input for the optimization phase. However, the loss function used to train the model may easily be misaligned with the end goal, which is to make the best decisions possible.  <br>
<a href="https://arxiv.org/pdf/1809.05504.pdf" rel="external nofollow noopener" target="_blank">Decision-focused learning frameworks</a> integrates prediction and optimization into a single end-to-end system whose predictive model is trained in such a way its predictions are optimal for the decision algorithm.<br>
The general optimization problem can be expressed as:</p>
<p align="center">
$$
   \underset{\omega}{\text{argmin}} \left\{ \sum_{i=1}^m c(z^*(y_i), \hat{y}_i) \mid y = f(\hat{x}, \omega) \right\}
$$
</p>

<p>where:</p>
<ul>
  <li>\(\hat{x}\) and \(\hat{y}\) are input-output pairs drawn from some unknown distribution;</li>
  <li>\(y\) the predictions of the model;</li>
  <li>\(\omega\) the parameters of a machine learning model;</li>
  <li>\(z\) the actions selected by the decision algorithm;</li>
  <li>\(c\) the cost function.</li>
</ul>

<p>Even if not explicitly presented, \(z^*\) is the solution of an optimization problem (the decision problem), which requires to solve an argmin/argmax operator which is non-differentiable. To overcome this problem, they proposed to work on a relaxation of the problem. Following this idea is possible to tackle this problem using standard reinforcement learning techniques in which the reward is the negative of the \(argmin\) argument of the previous equation.</p>

<h1 id="set-multicover-problem-">Set Multicover Problem <a name="section2"></a>
</h1>
<p>We define here the set multicover problem.<br>
Let (\(X, \mathcal{S}\)) be a set system, where $X$ is a finite ground set, \(\mathcal{S}\) is a collection of subsets of \(X\), and each element \(x \in X\) has a non-negative demand \(d(x)\). A set multicover problem requires picking the smallest cardinality sub-collection \(\mathcal{S}'\) of \(\mathcal{S}\) such that each point is covered by at least \(d(x)\) sets from \(\mathcal{S}\).</p>

<p>Some examples of set multicover problems are pharmacies locations, where you want to place pharmacies to cover all the demands of different locations, and project time scheduling where you want to allocate the available work time of employers to different project that requires certain amount of time to complete.</p>

<h1 id="soft-actor-critic-">Soft Actor Critic <a name="section3"></a>
</h1>
<p><a href="https://arxiv.org/pdf/1801.01290.pdf" rel="external nofollow noopener" target="_blank">Soft actor-critic</a>  is an off-policy actor-critic algorithm based on the maximum entropy framework; the term “soft” is derived from <a href="https://arxiv.org/pdf/1702.08165.pdf" rel="external nofollow noopener" target="_blank">soft Q-learning</a>. In this framework, the optimal policy is the one that optimizes at the same time both the expected cumulative reward value and the entropy of the policy. A parameter \(\alpha\) is introduced to regulate the importance of the entropy with respect to the reward.</p>

<p align="center"> 
$$
    \pi^* =   \underset\pi{\arg\max} \sum_{t=0}^T \mathbb{E}_{(s_t, a_t) \sim \rho_\pi}[\gamma^t (r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot | s_t))]
$$  
</p>

<p>Off-policy means that the systems uses data generated from a different policy that the one we are trying to optimize. Indeed, SAC exploits experience which is stored in a replay  buffer and from which batches are randomly sampled for improving the policy. After doing a policy update, the data is kept in the replay buffer leading to a mismatch between the policy that generated the data and the current policy.<br>
The system is composed of two soft Q-functions, two target soft Q-functions, and an action network. The action network is defined by a Gaussian whose mean and variance are computed by a neural network with parameter \(\phi\). The soft Q-functions are defined by two neural networks with parameters \(\theta_1\) and \(\theta_2\), while the target networks are parametrized with an exponential moving average of the parameters of the soft Q-functions as follow.</p>

\[\bar{\theta_i} \leftarrow (1 - \tau) \bar{\theta_i} + \tau \theta_i\]

<p>The presence of two soft Q-function helps to mitigate the effect of positive bias in the policy improvement step and moreover it helps to speed up the training. However only the minimum between the two soft Q-value is used for computing the gradients. The soft Q-function networks are trained to minimize the following equation..</p>

\[J_Q(\theta) = \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}}\left[ \frac{1}{2}\left(Q_{\theta}(s_t, a_t) - \left(r(s_t, a_t) + \gamma \ \mathbb{E}_{s_{t+1} \sim p}[V_{\bar{\theta}}(s_{t+1})]\right)\right)^2\right]\]

<p>In the first version of this algorithm was present a value-function approximator network which was then abandoned, since the value function is parametrized trough soft Q-function parameters using the following equation.</p>

\[V(s_t) = \mathbb{E}_{a_t \sim \pi} \left[Q(s_t, a_t) - \alpha \ log \ \pi(a_t|s_t)\right]\]

<p>Policy network is trained minimizing the following equation.</p>

\[J_{\pi}(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}} \left[
\ \mathbb{E}_{a_t \sim \pi_\phi} [\alpha \ log \ (\pi \phi(a_t|s_t)) - Q_\theta(s_t, a_t)]\right]\]

<p>For helping the optimization phase, made by backpropagation, a <a href="https://arxiv.org/pdf/1312.6114.pdf" rel="external nofollow noopener" target="_blank">reparametrization trick</a> is applied. 
Choosing the optimal temperature \(\alpha\) parameter is non-trivial and it need to be tuned for each task.<br>
Reward and entropy can vary a lot for each task and so also the temperature parameter that establish the relative importance of the two. Moreover entropy and rewards vary during training while the policy improve. In \cite{sac2}, they proposed an automated process for tuning this parameter formulating a maximum entropy reinforcement
learning objective, where the entropy is treated as a constraint.</p>
<p align="center"> 
$$
\underset{\pi 0:T}{\max} \ \mathbb{E}_{\rho_\pi} \left[ \sum_{t=0}^T \gamma^t  r(s_t, a_t) \right]
$$   
$$
s.t. \ \mathbb{E}_{(s_t,a_t) \sim \rho_\pi}
\left[- log(\pi_t(a_t|s_t))\right] \geq \mathcal{H},  \ \forall t
$$  
</p>

<p>where \(\mathcal{H}\) is the desired minimum expected entropy.<br>
Exploiting then the duality theorem, it’s possible to formulate the following equation for obtaining the optimal \(\alpha\).</p>

\[\alpha_t^* = \underset{\alpha_t}{arg min} \mathbb{E}_{a_t \sim \pi^*_t} \left[ - \alpha_t log \pi^*_t(a_t|s_t, \alpha_t) - \alpha_t \bar{\mathcal{H}}\right]\]

<p>where \(\bar{\mathcal{H}}\) is the target entropy.</p>

<h1 id="prioritized-experience-replay-">Prioritized Experience Replay <a name="section4"></a>
</h1>
<p><a href="https://arxiv.org/abs/1511.05952" rel="external nofollow noopener" target="_blank">Schaul et al.</a> proposed a new method for sampling the experience from the buffer. They suggested sampling with a higher probability the transitions from which is possible to learn more. A possible approximator of this value is the temporal difference error which in some reinforcement learning algorithms is already computed for updating the network parameters. However this greedy technique presents some issues; for example, it requires to sequentially pass all the replay buffer. To avoid expensive sweeps over the entire replay memory, TD errors are only updated for the transitions that are replayed. One consequence is that transitions that have a low TD error on the first visit may not be replayed for a long time (which means effectively never with a sliding window replay memory). Further, it is sensitive to noise spikes (e.g. when rewards are stochastic), which can be exacerbated by bootstrapping.<br>
To overcome this problem, they proposed sampling proportionally to the priority:</p>

\[P(i) = \frac{p_i^{\alpha}}{\sum_k p_i^{\alpha}}\]

<p>where \(p_i = |\delta| + \epsilon\) with \(\delta\) being TD error and \(\epsilon\) a small constant to avoid never revisiting states whose error was zero.<br>
The prioritized experience introduces bias which can be corrected using importance-sampling weights. These weights are then used for rescaling the gradient during the training. For stability reasons, the weights are normalized with respect to the maximum weight.<br>
The value \(\beta\) is increased till it reaches \(1\) toward the end.</p>

\[w_i = \left( \frac{1}{N} \cdot \frac{1}{P_i}\right)^{\beta}\]

<h1 id="experiments">Experiments</h1>
<p>We started our analysis with a quick comparison between SAC and two on-policy algorithms, <a href="https://arxiv.org/pdf/1707.06347.pdf" rel="external nofollow noopener" target="_blank">PPO</a> and VPG. This round of experiments was conducted using the same backbone for all the algorithms, a 2 fully-connected layers network with \(256\) neurons for each layer.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/algorithm_comparison-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/algorithm_comparison-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/algorithm_comparison-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/algorithm_comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

<p>From this first trial we noticed that on-policy algorithms converge faster but they are not capable of achieving the same reward value, so we decided to focus our attention on SAC.<br>
In our successive experiments, we tried to improve the reward and quicken the convergence.<br>
First of all we launched a grid search for identifying the best combination of hidden dimension and batch size, the batch size was chosen in the range \([128, 256, 512, 1024]\), while the hidden dimension was in the range \([128, 256, 512]\).<br>
From the first plot, we can notice that increasing the number of neurons for the layer helps the convergence, instead there isn’t a clear relationship between convergence and batch size since in some cases smaller batches converge faster. However, the best combination is the one with \(512\) as hidden dimension and \(1024\) as batch size. The reward evaluation plot is more difficult to read since the convergence at evaluation time is less stable than the one at training time.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/train_sac_comparison-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/train_sac_comparison-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/train_sac_comparison-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/train_sac_comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Comparison of train reward during the grid search.
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/eval_sac_comparison-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/eval_sac_comparison-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/eval_sac_comparison-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/eval_sac_comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Comparison of evaluation reward during the grid search.
</div>

<p>Once the range of possibilities was reduced, we experimented using the prioritized experience replay. We proposed two little variations to the standard technique, instead of using a liner annealing for the \(\beta\) value, we used an exponential one, and instead of assigning the maximum priority to the new samples, we noticed that assigning as priority the mean value of the priorities of the last sampled batch was more robust and improved the performances.<br>
After having fixed the batch size, the hidden dimension, and the type of buffer we launched a random search to find the best learning rate for both policy network and critic networks and the hyper-parameters related to the prioritized experience replay.<br>
The best configuration we found is presented in the table. This optimized version not only improved the convergence speed but also the reward value.
From the plot, we can notice that the SAC curve start after \(10000\) steps, this is due to the fact that the replay buffer requires a certain number of pre-collected samples before starting the training. We tried to reduce this number but decreasing it showed a drop in performances.<br>
Even if on-policy algorithm seems to converge first, they improve little by little during the whole training so they achieve their best evaluation reward after SAC whose evaluation reward is less stable but whose best results are faster.</p>

<table>
  <thead>
    <tr>
      <th><strong>Hyper-parameters</strong></th>
      <th><strong>Value</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Hidden dimension</td>
      <td>512</td>
    </tr>
    <tr>
      <td>Batch size</td>
      <td>1024</td>
    </tr>
    <tr>
      <td>Policy lr</td>
      <td>\(3*10^{-3}\)</td>
    </tr>
    <tr>
      <td>Critic lr</td>
      <td>\(3*10^{-2}\)</td>
    </tr>
    <tr>
      <td>Buffer \(\alpha\)</td>
      <td>0.6</td>
    </tr>
    <tr>
      <td>Starting \(\beta\)</td>
      <td>0.4</td>
    </tr>
    <tr>
      <td>Annealing rate \(\beta\)</td>
      <td>\(3*10^{-3}\)</td>
    </tr>
  </tbody>
</table>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/final_sac_comparison-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/final_sac_comparison-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/final_sac_comparison-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/final_sac_comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Comparison evaluation reward between the baseline, the best model without PER and the best with it.
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/final_comparison-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/final_comparison-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/final_comparison-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/final_comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Comparison of evaluation reward between PPO, VPG and the best SAC.
</div>

<h1 id="conclusions-">Conclusions <a name="section6"></a>
</h1>

<p>From our project, we can conclude that the decision focused learning framework it’s a promising direction for solving decision making problem.<br>
Regarding the analyzed algorithms, it seems that on-policy algorithms converge faster and it’s particularly important in this context since faster convergence means being able to provide faster solutions to the decision problem. On the other hand SAC delivers better performances reducing the cost of the decision problem.<br>
After the introduction on the prioritized experience replay technique, the convergence of SAC speeds up, proving the technique to be effective also in this context.<br>
Moreover we have to take in account that the first \(10000\) steps, required for filling the buffer, are just explorative steps and they are quicker to execute than the latter ones which also include the policy update phase.   In the end, we can affirm to be satisfied by our project since we achieved better reward, while the increased amount of steps required for the solution seems to be acceptable with respect to the starting phase.<br>
A possible future improvements for this project would be the implementation of <a href="https://arxiv.org/pdf/1905.12726.pdf" rel="external nofollow noopener" target="_blank">Prioritized Sequence Experience Replay</a>.</p>


          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Mattia  Bertè. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
