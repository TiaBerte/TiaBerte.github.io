<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Masked Face recognition | Mattia  Bertè</title>
    <meta name="author" content="Mattia  Bertè">
    <meta name="description" content="Masked Face recogntion problem solved using Barlow Twins technique">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tiaberte.github.io/projects/masked-face/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mattia </span>Bertè</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Masked Face recognition</h1>
            <p class="post-description">Masked Face recogntion problem solved using Barlow Twins technique</p>
          </header>

          <article>
            <h1 id="project-description">Project description</h1>
<p>When Covid-19 appeared, the most immediate defensive tool for contrasting the spread was the use of face-mask. However this introduction made more evident one of the weaknesses of our facial recognition systems, they indeed were unable to correctly classify/recognize the identity of a person wearing a face mask. We tried to overcome this problem proposing a solution applicable to the standard systems made by a convolutional neural network as feature extractor plus a final classifier such as SVM or \(k\)-NN. Our method exploits the Barlow Twins technique for learning the invariance to the presence of the face mask. We used MLFW dataset, a new synthetic dataset created starting from CALF, so our network not only learned the invariance to the face mask but also to the age. Our results aren’t comparable to state-of-the-art systems however seem promising for successive studies.<br>
The code can be found on <a href="https://github.com/TiaBerte/masked-face" rel="external nofollow noopener" target="_blank">GitHub</a>.</p>

<h1 id="table-of-contents">Table of Contents</h1>
<ol>
  <li><a href="#section1">Problem definition</a></li>
  <li><a href="#section2">Proposed Solution</a></li>
  <li><a href="#section3">Barlow Twins</a></li>
  <li><a href="#section4">Dataset</a></li>
  <li><a href="#section5">Distributed optimization</a></li>
  <li><a href="#section6">Experiments and results</a></li>
  <li><a href="#section7">Ethical issues</a></li>
  <li><a href="#section8">Conclusions</a></li>
</ol>

<h1 id="problem-definition--">Problem definition  <a name="section1"></a>
</h1>
<p>When Covid-19 disease started to spread all over the world, face-mask become one of the first defensive strategy to counteract its diffusion. Face recognition is one of the most common biometric authentication methods,  however masked face recognition is a highly challenging task since the mask occludes partially the face making impossible extracting some informative features. Previous systems indeed showed a drop in performances up to \(20\%\) when they had to deal with a masked face. <br>
The most widespread solutions for facial recognition/identification were based on a features extraction phase, usually made by a convolutional neural network trained in a supervised way, plus a final classifier such as SVM or \(k\)-NN.</p>

<h1 id="proposed-solution--">Proposed Solution  <a name="section2"></a>
</h1>
<p>We tried to propose our solution exploiting a self-supervised learning technique called <a href="https://arxiv.org/pdf/2103.03230.pdf" rel="external nofollow noopener" target="_blank">Barlow Twins</a>. The idea is pretty simple, they proposed to pass through a twins network architecture two distorted versions of the same image obtained by applying some randomly selected transformation from a set of predefined ones, and then with the help of a defined ad hoc loss, the system learns to produce the same embedding for the distorted images. It helps to produce very similar embedding for all the images of the same class since the network becomes invariant to all these transformations.<br>
Starting from this idea we thought to consider wearing a face mask as a transformation and to train the network to learn the invariance to the presence of the mask.<br>
At each step, the network receives two images of the same person, one with the mask and one without the mask. We found a tool for generating realistic masked-face starting from unmasked ones, however, to decrease the computational costs we used the pre-built dataset obtained using this tool.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/maskedface-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/maskedface-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/maskedface-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/maskedface.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Face-mask application pipeline.
</div>

<h1 id="barlow-twins--">Barlow Twins  <a name="section3"></a>
</h1>
<p>This section contains a brief theoretical review of the Barlow Twins’ loss.<br>
This technique produces two distorted views for all images of a batch sampled from a dataset. The distorted views are obtained via a distribution of data augmentations. The two batches of distorted views \(Y^A\) and \(Y^B\) are then fed to a neural network which generates embedding \(Z^A\) and \(Z^B\) respectively.<br>
They defined the following new loss.</p>

\[\mathcal{L}_{BT} = \sum_i (1 - \mathcal{C}_{ii})^2 + \lambda \sum_i \sum_{j \neq i} C_{ij}^2\]

<p>where \(\lambda\) is a positive constant trading off the importance of the first and second terms of the loss, and where $\mathcal{C}$ is the cross-correlation matrix computed between the outputs of the two identical networks along the batch dimension:</p>

\[\mathcal{C}_{ij} = \frac{\sum_b z_{b,i}^A z_{b,j}^B}{\sqrt{\sum_b (z_{b,i}^A)^2} \sqrt{\sum_b (z_{b,i}^B)^2}}\]

<p>where \(b\) indexes batch samples and \(i, j\) index the vector dimension of the networks’ outputs. \(\mathcal{C}\) is a square matrix with the size the dimensionality of the network’s output, and with values comprised between \(-1\) (i.e. perfect anti-correlation) and \(1\) (i.e. perfect correlation).<br>
Intuitively, the invariance term of the objective, by trying to equate the diagonal elements of the cross-correlation matrix to \(1\), makes the embedding invariant to the distortions applied. The redundancy reduction term, by trying to equate the off-diagonal elements of the cross-correlation matrix to $0$, decorrelates the different vector components of the embedding. This decorrelation reduces the redundancy between output units so that the output units contain non-redundant information about the sample.</p>

<h1 id="dataset--">Dataset  <a name="section4"></a>
</h1>
<p>As explained in Section 2, the dataset we used is a synthetic one, called <a href="https://arxiv.org/pdf/2109.05804.pdf" rel="external nofollow noopener" target="_blank">MLFW (Masked LFW)</a>. They started from <a href="https://arxiv.org/pdf/1708.08197.pdf" rel="external nofollow noopener" target="_blank">CALFW (Cross-Aged LFW)</a> dataset which is a variation of the widespread dataset <a href="http://vis-www.cs.umass.edu/lfw/lfw.pdf" rel="external nofollow noopener" target="_blank">LFW (Labelled Face in the Wild)</a>. CALFW contains labeled faces of famous people in different scenarios, moreover, people are present at different ages making the classification harder.<br>
This fact made our task more complex since the network did not need just to learn the invariance to the face mask but also to the changes due to the age, making in a lot of cases the comparison between colored and gray-scale images. The dataset required also some cleaning due to duplicated images, similar masks applied to the same image, or bad-quality images.
Starting from these images, they created a tool for generating realistic masked face through a complex algorithm that detects some key points and then applies the mask. To make more general as possible, researchers applied different mask templates to mimic the most common face masks.<br>
Barlow Twins requires big batch sizes to perform well, however large batch sizes are known to lead to a drop in performance. To overcome this problem, they used <a href="https://arxiv.org/pdf/1708.03888.pdf" rel="external nofollow noopener" target="_blank">LARS</a> as an optimizer which allows the use of large batch size thanks to different adaptive learning rates for each layer.</p>
<div class="row">
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Frank_Abagnale_Jr_0002_0000-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Frank_Abagnale_Jr_0002_0000-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Frank_Abagnale_Jr_0002_0000-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Frank_Abagnale_Jr_0002_0000.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Roseanne_Barr_0004_0000-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Roseanne_Barr_0004_0000-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Roseanne_Barr_0004_0000-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Roseanne_Barr_0004_0000.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="row">
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Tatiana_Panova_0004_0000-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Tatiana_Panova_0004_0000-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Tatiana_Panova_0004_0000-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Tatiana_Panova_0004_0000.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Paula_Dobriansky_0001_0000-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Paula_Dobriansky_0001_0000-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Paula_Dobriansky_0001_0000-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Paula_Dobriansky_0001_0000.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Example of low-quality images.
</div>

<h1 id="distributed-optimization-">Distributed optimization <a name="section5"></a>
</h1>
<p>To speed up this computationally expensive training, we split the work on 8 Quadro RTX 6000.<br>
Each GPU processes a different subset of the batch and computes the gradient; then all the gradients are averaged together to obtain the final update direction. In particular, we used a batch size of \(512\), thus every GPU process \(64\) samples. To speed up the communication of the gradient, we tried <a href="https://arxiv.org/pdf/1905.13727.pdf" rel="external nofollow noopener" target="_blank">PowerSGD</a>, a gradient compression algorithm. PowerSGD compresses the gradient using a low-rank approximation. For speed purposes, the low-rank approximation is estimated using the power iteration algorithm.</p>

<h1 id="experiments-and-results-">Experiments and results <a name="section6"></a>
</h1>
<p>We finetuned a ResNet50 pre-trained on <a href="https://arxiv.org/pdf/1607.08221.pdf" rel="external nofollow noopener" target="_blank">MS-Celeb-1M</a> and already finetuned on <a href="https://arxiv.org/pdf/1710.08092.pdf" rel="external nofollow noopener" target="_blank">VGG2</a> dataset (weights can be found on {GitHub](https://github.com/cydonia999/VGGFace2-pytorch). <br>
We tested the network on identities not present in the train set. We split the test set into 2 parts, one for training the \(k\)-NN and one for testing it.<br>
We generated the embedding for the $k$-NN using the backbone. We selected \(k = 1\), which means that the predicted identity is one of the closest embedding in the projected space, it was also due to the fact that many identities present just a few images, so using \(k=1\) treat all the identities in the same way without favoring the ones with many images.<br>
Since SGD presented the best results, we tried to exploit at most all the images at our disposal. Instead of sampling only 2 images for each celebrities, we sampled all \(n/2\) couples for each identity where \(n\) is the number of image for that id. Doing so the number of couples for each epochs almost doubled.<br>
In Table are presented the results of our experiments.</p>

<table>
  <thead>
    <tr>
      <th><strong>Technique</strong></th>
      <th><strong>Accuracy</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SGD</td>
      <td>79.72</td>
    </tr>
    <tr>
      <td>PowerSGD</td>
      <td>74.72</td>
    </tr>
    <tr>
      <td>SGD + new sampler</td>
      <td>76.67</td>
    </tr>
  </tbody>
</table>

<h1 id="ethical-issues-">Ethical issues <a name="section7"></a>
</h1>
<p>Face identification presents always some issues related to the privacy of data and to the possibility of discrimination due to the different capabilities of the system to identify persons of different ethnicity. This is usually related to the unbalanced distribution of the training set.<br>
We tried to derive the ethnicity of the people using an automatic method, such as <a href="https://ieeexplore.ieee.org/document/8266229" rel="external nofollow noopener" target="_blank">skin tone estimation</a>. This algorithm (repo on <a href="https://github.com/colin-yao/simple-skin-detection" rel="external nofollow noopener" target="_blank">GitHub</a>)consists of two main parts :</p>

<ul>
  <li>Foreground and background separation using Otsu’s Binarization;</li>
  <li>Pixel-wise skin classifier based on HSV and YCrCb colorspaces.</li>
</ul>

<p>Even if the algorithm performed very well, it was difficult to cluster the images, indeed the tone varies really gradually and it is impossible to estimate the ethnicity depending only on this feature. Moreover, the skin tone was too dependent on the light condition.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/BGR-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/BGR-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/BGR-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/BGR.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/final_skin-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/final_skin-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/final_skin-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/final_skin.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/color-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/color-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/color-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/color.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Example of skin tone estimation.
</div>

<p>We decided to analyze the performances of our system in each different group. We identified $5$ main groups: White/Caucasian, African American, Asiatic, Arab, and Hispanic.<br>
We labeled each person manually, however, this has not to be intended as a classification aim to create a new dataset but just to provide an idea of the dataset distribution. We are conscious of the possibility of making mistakes in this labeling process, our purpose was just for performance analysis and it wasn’t intended to offend any of the people on the list.</p>

<table>
  <thead>
    <tr>
      <th><strong>Ethnicity</strong></th>
      <th><strong>Train set percentage</strong></th>
      <th><strong>Test set percentage</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>African American</td>
      <td>8.04</td>
      <td>5.22</td>
    </tr>
    <tr>
      <td>Arab</td>
      <td>4.06</td>
      <td>6.71</td>
    </tr>
    <tr>
      <td>Asiatic</td>
      <td>5.49</td>
      <td>2.24</td>
    </tr>
    <tr>
      <td>Hispanic</td>
      <td>5.11</td>
      <td>6.72</td>
    </tr>
    <tr>
      <td>Caucasian</td>
      <td>77.30</td>
      <td>79.11</td>
    </tr>
  </tbody>
</table>

<p>Once the dataset was labeled, we checked the performances of the single ethnicity to prevent biases. The performances are reported in the following table.</p>

<table>
  <thead>
    <tr>
      <th><strong>Ethnicity</strong></th>
      <th><strong>SGD</strong></th>
      <th><strong>PowerSGD</strong></th>
      <th><strong>SGD + new sampler</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>African American</td>
      <td>94.44</td>
      <td>94.44</td>
      <td>88.89</td>
    </tr>
    <tr>
      <td>Arab</td>
      <td>75.00</td>
      <td>85.72</td>
      <td>78.57</td>
    </tr>
    <tr>
      <td>Asiatic</td>
      <td>83.33</td>
      <td>83.33</td>
      <td>83.33</td>
    </tr>
    <tr>
      <td>Hispanic</td>
      <td>80.00</td>
      <td>70.00</td>
      <td>70.00</td>
    </tr>
    <tr>
      <td>Caucasian</td>
      <td>79.02</td>
      <td>72.72</td>
      <td>75.87</td>
    </tr>
  </tbody>
</table>

<p>Unexpectedly even if the Caucasian ethnicity is the most present, it isn’t the one that presents the best accuracy. It means that this approach isn’t particularly biased toward some specific ethnicity or it’s unable to work with a specific minority.</p>

<h1 id="conclusions-">Conclusions <a name="section6"></a>
</h1>
<p>We weren’t interested in reaching state-of-the-art performances in masked face recognition, however, we proved the validity of our idea by exploiting a self-supervised technique combined with a metric learning approach. Our proposed solution first of all showed the effectiveness of the Barlow Twins technique moreover, we understood that also face masks and aging can be considered as simple transformations as it is for geometric or color transformations.<br>
One of the biggest weaknesses of our solution is related to the dataset, it’s not built so well for our purpose. It required some manual cleaning and a lot of images was really similar since produced from the same original unmasked face with the addition of the same face mask but with different colors. It slows down the learning process and the generalization capability. This can be also the reason behind the decreasing in performances when we increased the dataset size. Sampling all the possible couple increased a lot the probability of sampling two really similar images which causes bad training.<br>
Even if PowerSGD seemed a promising technique for speeding up the training, we noticed little to no speedup in using it. We believe this is because the GPUs were located on the same node, thus they had a very high bandwidth.<br>
In the end, a positive aspect we think is worth noticing is the behavior related to the different ethnicity. Of course, we can notice different performances for each subgroup, however, the majority of them show better performances than the Caucasian ethnicity. We cannot conclude for sure that this approach is free from biases however it seems a promising point for successive studies.</p>

<div class="row">
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Michael_Jordan_0001_0000-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Michael_Jordan_0001_0000-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Michael_Jordan_0001_0000-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Michael_Jordan_0001_0000.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Michael_Jordan_0001_0003-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Michael_Jordan_0001_0003-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Michael_Jordan_0001_0003-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Michael_Jordan_0001_0003.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Example of similar images produced from the same original one.
</div>


          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Mattia  Bertè. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
